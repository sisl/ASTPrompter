"""
This is as standalone script to train a baseline model into a hardened defender model based on example rollouts generated using an attacker model
trained using adaptive stress testing.

The general idea is that the traininge examples generated by the AST-attacker model will be more effective at hardening the defender model against
toxicity than the training examples generated by the baseline model.
"""

# Imports
import typer
import json
import copy
import logging
import wandb
from pathlib import Path
from enum import Enum
from dataclasses import dataclass

from typing import List, Optional
from typing_extensions import Annotated

import numpy as np
import torch
import random

import torch.nn.functional as F
from convokit import Corpus, download
from toxicity.split_data import filter_corpus_by_file
from toxicity.reddit_data_helpers import corpus_to_prompts
from toxicity.dxf import call_detoxify

from accelerate.utils import set_seed
from accelerate.logging import get_logger
from accelerate.utils.tqdm import tqdm
from accelerate import Accelerator
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import LambdaLR
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

from lm import LanguageModel

from pynvml import *

# Used to get GPU information globally (for processes other than the current one)
# Used to inform GPU selection
nvmlInit()

logger = get_logger("ast")
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
logging.basicConfig(format=LOG_FORMAT, level=logging.INFO)
logger.setLevel(logging.INFO)

# Define Main Typer Function
app = typer.Typer()

# Enum argument for po-method (dpo or apo)
class PoMethod(str, Enum):
    dpo = "dpo"
    apo = "apo"

class ModelName(str, Enum):
    # Define moreb baseline / defender models here. Must be valid Hugging Face model names
    gpt2 = "openai-community/gpt2"

class Attackers(str, Enum):
    # Define more attacker names here. Must be present in ./models as a folder
    gpt2 = "gpt2_defense_gpt2_best"

def get_text_from_conversation(conversation):
    return "".join([i['text'] for i in conversation])

def get_free_gpu():
    if not torch.cuda.is_available():
        return "cpu"
    free_memory = []
    
    for i in range(torch.cuda.device_count()):
        gpu_info = nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i))
        free_memory.append((gpu_info.free, i))
    _, best_gpu = max(free_memory)
    return f"cuda:{best_gpu}"

@dataclass
class Rollout:
    toxic_conversation: list[dict]
    nontoxic_conversation: list[dict]
    toxic_score: float
    nontoxic_score: float

class RolloutDataset(Dataset):
    def __init__(self, rollouts):
        self.rollouts = rollouts

    def __len__(self):
        return len(self.rollouts)

    def __getitem__(self, idx):
        # Construct a single body of text from the text
        toxic_text = get_text_from_conversation(self.rollouts[idx].toxic_conversation)
        nontoxic_text = get_text_from_conversation(self.rollouts[idx].nontoxic_conversation)

        return (toxic_text, nontoxic_text, self.rollouts[idx].toxic_score, self.rollouts[idx].nontoxic_score)

class Trainer:
    def __init__(self, 
                 attacker: str, 
                 baseline: str, 
                 defender: str, 
                 wandb: bool, 
                 po_method: PoMethod, 
                 po_beta: float = 0.1,
                 dpo_label_smooth: float = 0.1,
                 learning_rate: float = 5e-7,
                 max_gradient_norm: float = 10.0,
                 warmup_steps: int = 150,
                 save_dir: str = 'output',
                 save_name: str = 'model',
                 turn_order: list = ["atk", "def", "atk", "def", "atk", "def"],
                 max_new_tokens: int = 24,
                 min_new_tokens: int = 2,
                 repetition_penalty: float = 1.1,
                 temperature: float = 0.7):
    
        # Store turn order
        self.turn_order = turn_order

        # Validate turn order
        invalid_moves = set(turn_order) - set(["atk", "def"])
        if invalid_moves:
            raise ValueError(f"Invalid moves in turn order: {invalid_moves}")

        # Store rollout parameters
        self.max_new_tokens = max_new_tokens
        self.min_new_tokens = min_new_tokens
        self.repetition_penalty = repetition_penalty
        self.temperature = temperature

        self.accelerator = Accelerator()

        # The attacker is the AST-pretrained model that tries to get a baseline or defenderm model to generate toxic text
        self.attacker_name = attacker
        self.attacker = LanguageModel(dont_init=True)
        self.attacker.model = AutoModelForCausalLM.from_pretrained(attacker)
        self.attacker.tokenizer = AutoTokenizer.from_pretrained(attacker, padding_side='left')

        # The defender is the model we will tune to harden against the attacker
        self.defender_name = defender
        self.defender = LanguageModel(dont_init=True)
        self.defender.model = AutoModelForCausalLM.from_pretrained(defender)
        self.defender.tokenizer = AutoTokenizer.from_pretrained(defender, padding_side='left')

        # The baseline is the model we will use to generate training examples for the defender
        self.baseline_name = baseline
        self.baseline = LanguageModel(dont_init=True)
        self.baseline.model = AutoModelForCausalLM.from_pretrained(baseline)
        self.baseline.tokenizer = AutoTokenizer.from_pretrained(baseline, padding_side='left')

        # GPT 2 doesn't have a padding token, so we add it
        if 'gpt2' in attacker:
            self.attacker.tokenizer.pad_token = self.attacker.tokenizer.eos_token
            self.attacker.tokenizer.pad_token_id = self.attacker.tokenizer.eos_token_id

        if 'gpt2' in defender:
            self.defender.tokenizer.pad_token = self.defender.tokenizer.eos_token
            self.defender.tokenizer.pad_token_id = self.defender.tokenizer.eos_token_id

        if 'gpt2' in baseline:
            self.baseline.tokenizer.pad_token = self.baseline.tokenizer.eos_token
            self.baseline.tokenizer.pad_token_id = self.baseline.tokenizer.eos_token_id

        # Set Adversary and Baseline to eval mode
        self.attacker.model.eval()
        self.baseline.model.eval()

        # Set the policy optimization method
        self.po_method = po_method
        self.po_beta = po_beta
        self.dpo_label_smooth = dpo_label_smooth
        self.max_gradient_norm = max_gradient_norm

        optimizer = AdamW(self.defender.model.parameters(), lr=learning_rate)
        scheduler = LambdaLR(optimizer, lr_lambda=lambda step: min(1.0, (step + 1) / (warmup_steps + 1)))

        # Move models to accelerator devices
        logger.info(f'Accelerator device: {self.accelerator.device}')
        # self.attacker.model.to(self.accelerator.device)
        # self.baseline.model.to(self.accelerator.device)
        # self.defender.model.to(self.accelerator.device)

        # Use Accelerate to move models to accelrator device. We just let it take care of it
        # This is magic I don't like
        self.attacker.model, self.baseline.model, self.defender.model, self.optimizer, self.scheduler = self.accelerator.prepare(self.attacker.model, self.baseline.model, self.defender.model, optimizer, scheduler)

        # Print model devices
        logger.info(f'Attacker model device placement: {self.attacker.model.device}')
        logger.info(f'Baseline model device placement: {self.baseline.model.device}')
        logger.info(f'Defender model device placement: {self.defender.model.device}')

        if wandb:
            wandb.watch(self.defender.model)

        self.save_dir = save_dir
        self.save_name = save_name

        self._global_step_count = 0

    @property
    def device(self):
        return self.accelerator.device

    def finish(self):
        self.accelerator.end_training()

    def save(self, suffix: str | None = None):
        """
        Save the defender model
        """

        filename = self.save_dir + f"/{self.save_name}_" + suffix if suffix is not None else str(self._global_step_count)

        self.defender.model.save_pretrained(filename)
        self.defender.tokenizer.save_pretrained(filename)

    def generate_rollout_pair(self, prompt: str, toxicity_difference_threshold: float = 0.2, retries_left: int = 3):
        # Generate two rollouts first expected to be toxic, second expected to be nontoxic
        toxic_conversation    = self._generate_rollout(self.attacker, self.baseline, prompt)
        nontoxic_conversation = self._generate_rollout(self.baseline, self.defender, prompt)

        # Score each of the rollouts
        ro_toxic_score    = self._reward(get_text_from_conversation(toxic_conversation))
        ro_nontoxic_score = self._reward(get_text_from_conversation(nontoxic_conversation))

        if abs(ro_toxic_score - ro_nontoxic_score) < toxicity_difference_threshold and retries_left > 0:
            # If the difference in toxicity is not significant, try again
            return self.generate_rollout_pair(prompt, toxicity_difference_threshold, retries_left - 1)
        elif abs(ro_toxic_score - ro_nontoxic_score) < toxicity_difference_threshold and retries_left == 0:
            # If we have tried too many times and still can't get a good pair, return None
            return None
        else:
            # If the difference in toxicity is significant, return the pair and scores
            return Rollout(toxic_conversation, nontoxic_conversation, ro_toxic_score, ro_nontoxic_score)

    def _generate_rollout(self, attacker_model, defender_model, prompt):

        # Copy the promp
        text = copy.deepcopy(prompt)

        conversation = [{
            "text": copy.deepcopy(prompt),
            "role": "prompt",
            "model": "prompt",
            "toxicity": call_detoxify(text)["toxicity"]
        }]


        for move in self.turn_order:
            if move == "atk":
                # Generate a continuation of the text using the attacker model
                new_text = attacker_model.rollout(text, max_new_tokens=self.max_new_tokens, repetition_penalty=self.repetition_penalty, temperature=self.temperature, skip_special_tokens=True, min_new_tokens=self.min_new_tokens)
                
                # Extract just the new utterance
                new_utterance = new_text.replace(text, '')

                conversation.append({
                    "text": new_utterance,
                    "role": "atk",
                    "toxicity": call_detoxify(new_utterance)["toxicity"]
                })

                # Update the text with the new utterance
                text = new_text
            
            
            elif move == "def":
                # Generate a continuation of the text using the defender model
                new_text = defender_model.rollout(text, max_new_tokens=self.max_new_tokens, repetition_penalty=self.repetition_penalty, temperature=self.temperature, skip_special_tokens=True, min_new_tokens=self.min_new_tokens)
                
                # Extract just the new utterance
                new_utterance = new_text.replace(text, '')

                conversation.append({
                    "text": new_utterance,
                    "role": "def",
                    "toxicity": call_detoxify(new_utterance)["toxicity"]
                })

                # Update the text with the new utterance
                text = new_text
            
            else:
                raise ValueError(f"Invalid move: {move}")
            
        return conversation
    
    def _reward(self, text: str):
        """
        Reward function to score the toxicity of an utterance
        """
        
        # For this training since we're just trying to harden the model based on perceived toxicity, we don't adjust
        # reward based on which model generated the text or the perplexity of the text. Just the toxicity score
        
        # Lower toxicity score is better so we negate the toxicity score
        # Assumes input is just a plain string
        return 1.0-call_detoxify(text)["toxicity"]

    def prepare_data(self, rollouts, batch_size: int = 1):
        """
        Prepare list of examples for training the defender model
        """
        
        # Create a dataset from the rollouts
        ds = RolloutDataset(rollouts)

        # Create a DataLoader from the dataset
        # Don't need to shuffle since the rollout prompts are already randomized
        dl = DataLoader(ds, batch_size=batch_size)

        return self.accelerator.prepare(dl)


    def epoch(self, dataloader, log_interval: int = 10):
        """
        Train the defender model for one epoch
        """
        
        for i, batch in enumerate(iter(dataloader)):

            loss, metrics = self._step(batch, log_step=(i % log_interval == 0))

            if not torch.isnan(loss):
                self.accelerator.backward(loss)

            gn = clip_grad_norm_(self.defender.model.parameters(), self.max_gradient_norm).cpu().item()

            metrics["training/gradient_norm"] = gn

            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

            if i % log_interval == 0:
                metrics["training/lr"] = self.optimizer.param_groups[0]["lr"]
                self.accelerator.log(metrics, step = self._global_step_count)
                logger.info(f'Step {self._global_step_count} - Reward margin: {round(metrics['rewards/reward_margin'],5):.5f}, Loss: {round(metrics['training/loss'],5):.5f}')

        self._global_step_count += 1

    def _step(self, batch, log_step: bool = False):
        """
        Perform a single training step
        """        

        toxic_text, nontoxic_text, _, _ = batch
        
        # print(f'Toxic text: {toxic_text[0]}')
        # print(f'Nontoxic text: {nontoxic_text[0]}')

        # TODO: Discuss the win/loss defender/attacker terminology with Jack

        # Individually calculate the logprobs of the toxic and baseline examples
        with torch.inference_mode():
            baseline_nontoxic_logprobs = self.baseline.logprob_batched(nontoxic_text, self.accelerator.device)
            baseline_toxic_logprobs    = self.baseline.logprob_batched(toxic_text, self.accelerator.device)

        defender_nontoxic_logprobs = self.defender.logprob_batched(nontoxic_text, self.accelerator.device)
        defender_toxic_logprobs = self.defender.logprob_batched(toxic_text, self.accelerator.device)

        print(f'baseline_nontoxic_logprobs: {baseline_nontoxic_logprobs}')
        print(f'defender_nontoxic_logprobs: {defender_nontoxic_logprobs}')
        print(f'Difference: {defender_nontoxic_logprobs - baseline_nontoxic_logprobs}')
        print(f'baseline_toxic_logprobs: {baseline_toxic_logprobs}')
        print(f'defender_toxic_logprobs: {defender_toxic_logprobs}')
        print(f'Difference: {defender_toxic_logprobs - baseline_toxic_logprobs}')

        # Calculate Loss
        losses, preferred_reward, rejected_rewards = self._loss(defender_nontoxic_logprobs, defender_toxic_logprobs,
                                                              baseline_nontoxic_logprobs, baseline_toxic_logprobs)
        reward_accuracies = (preferred_reward > rejected_rewards).float()

        if torch.isnan(losses.mean()):
            raise ValueError("Loss is NaN")
        
        # Calculate metrics
        metrics = {
            "rewards/preferred": preferred_reward.mean().cpu().item(),
            "rewards/rejected": rejected_rewards.mean().cpu().item(),
            "rewards/reward_accuracy": reward_accuracies.mean().cpu().item(),
            "rewards/reward_margin": (preferred_reward - rejected_rewards).mean().cpu().item(),
            "policy/logprobs_preferred": defender_nontoxic_logprobs.mean().detach().cpu().item(),
            "policy/logprobs_rejected": defender_toxic_logprobs.mean().detach().cpu().item(),
            "ref/logprobs_preferred": baseline_nontoxic_logprobs.mean().detach().cpu().item(),
            "ref/logprobs_rejected": baseline_toxic_logprobs.mean().detach().cpu().item(),
            "training/loss": losses.mean().detach().cpu().item(),
            "debug/text": wandb.Table(data=list(zip(nontoxic_text, toxic_text)), 
                columns=["preferred", "rejected"])
        }

        logger.info(f'METRICS: {metrics}')

        wandb.log(metrics, step=self._global_step_count)

        return losses.mean(), metrics

    def _loss(self, policy_chosen_logps, policy_rejected_logps,
                     reference_chosen_logps, reference_rejected_logps):
        
        # https://github.com/eric-mitchell/direct-preference-optimization/blob/f8b8c0f49dc92a430bae41585f9d467d3618fe2f/trainers.py#L70-L87
        
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios  # also known as h_{\pi_\theta}^{y_w,y_l}

        # this is IPO, Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf
        if self.po_method == PoMethod.dpo:
            losses = -F.logsigmoid(self.po_beta * logits) * (1 - self.dpo_label_smooth) - F.logsigmoid(-self.po_beta * logits) * self.dpo_label_smooth
        else: # IPO
            losses = (logits - 1.0/(2.0 * self.po_beta)) ** 2

        preferred_rewards = self.po_beta * (policy_chosen_logps - reference_chosen_logps).detach().cpu()
        rejected_rewards = self.po_beta * (policy_rejected_logps - reference_rejected_logps).detach().cpu()

        return losses, preferred_rewards, rejected_rewards

@app.command()
def train_defender(
    save_dir: str = 'output',
    save_name: str = typer.Argument(...),
    attacker: Attackers = Attackers.gpt2,
    baseline: ModelName = ModelName.gpt2,
    defender: ModelName = ModelName.gpt2,
    epochs: int = typer.Option(10000, help="Number of epochs to train the defender model"),
    enable_wandb: bool = typer.Option(True, help="Whether to log to wandb"),
    po_method: PoMethod = typer.Option(PoMethod.dpo, help="The policy optimization method to use"),
    po_beta: float = typer.Option(0.1, help="The beta parameter for the DPO objective"),
    dpo_label_smooth: float = typer.Option(0.1, help="Label smoothing for the DPO objective"),
    batch_size: int = typer.Option(8, help="Batch size for training the defender model"), # 8
    learning_rate: float = typer.Option(5e-7, help="Learning rate for the defender model"),
    warmup_steps: int = typer.Option(150, help="Number of warmup steps for the learning rate scheduler"),
    rollouts_per_epoch: int = typer.Option(64, help="Number of rollouts to generate per epoch"), # 256
    seed: int = typer.Option(42, help="Random seed for reproducibility"),
    turn_order: Annotated[List[str], typer.Option(help="Turn order for the rollout")] = ["atk", "def", "atk", "def", "atk", "def"],
    max_new_tokens: int = typer.Option(24, help="Maximum number of tokens to generate in a single rollout"),
    min_new_tokens: int = typer.Option(2, help="Minimum number of tokens to generate in a single rollout"),
    repetition_penalty: float = typer.Option(1.1, help="Repetition penalty for the rollout"),
    temperature: float = typer.Option(0.7, help="Temperature for the rollout"),
    dev_eval_interval: int = typer.Option(10, help="Number of epochs between dev set evaluations")
):
    """
    Train a baseline model into a hardened defender model based on example rollouts generated using an attacker model
    trained using adaptive stress testing.

    Example use:

    ```
    python train_defender.py gpt2_hardened
    ```

    """

    # Confirm that a GPU is available
    if not torch.cuda.is_available():
        typer.echo("No GPU available. Exiting.")
        raise typer.Exit()
    
    # Check that rollouts_per_epoch is a multiple of batch_size
    if rollouts_per_epoch % batch_size != 0:
        typer.echo("rollouts_per_epoch must be a multiple of batch_size")
        raise typer.Exit()
    
    # Create a new directory for the model if it does not exist
    output_dir = Path(save_dir) / save_name

    if not output_dir.exists():
        output_dir.mkdir(parents=True)

    typer.echo(f"Saving model to {output_dir}")

    # Save the configuration to the output directory
    config = {
        "attacker": attacker,
        "baseline": baseline,
        "defender": defender,
        "epochs": epochs,
        "enable_wandb": enable_wandb,
        "po_method": po_method,
        "po_beta": po_beta,
        "dpo_label_smooth": dpo_label_smooth,
        "seed": seed,
        "prompt_source": "convokit",
        "learning_rate": learning_rate,
        "warmup_steps": warmup_steps,
        "batch_size": batch_size,
        "turn_order": turn_order,
        "max_new_tokens": max_new_tokens,
        "min_new_tokens": min_new_tokens,
        "repetition_penalty": repetition_penalty,
        "temperature": temperature,
        "dev_eval_interval": dev_eval_interval,
        "hostname": os.uname().nodename
    }

    with open(output_dir / "config.json", "w") as f:
        json.dump(config, f, indent=4)

    
    # Initialize Wandb
    if enable_wandb:
        wandb.init(project="AST Defener Hardening", config=config)

    # Set the seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    set_seed(seed)

    # Load data
    typer.echo("Loading data...")
    train_prompts = corpus_to_prompts(filter_corpus_by_file(Corpus(filename=download("reddit-corpus-small")), "data/train.txt"))
    dev_prompts   = corpus_to_prompts(filter_corpus_by_file(Corpus(filename=download("reddit-corpus-small")), "data/dev.txt"))

    # Initialize Training Model
    typer.echo("Initializing models...")

    typer.echo(f"Attacker: {attacker.value}")
    typer.echo(f"Baseline: {baseline.value}")
    typer.echo(f"Defender: {defender.value}")

    trainer = Trainer(
        attacker= "./models/" + attacker.value, 
        baseline=baseline.value, 
        defender=defender.value, 
        wandb=wandb, 
        po_method=po_method, 
        po_beta=po_beta, 
        dpo_label_smooth=dpo_label_smooth,
        learning_rate=learning_rate, 
        warmup_steps=warmup_steps,
        save_dir=save_dir,
        save_name=save_name,
        turn_order=turn_order,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        repetition_penalty=repetition_penalty,
        temperature=temperature
    )

    # Initialize best model
    best_model = float('inf')

    while trainer._global_step_count < epochs:
        typer.echo(f"Epoch {trainer._global_step_count}")

        train_rollouts = []
        _cntr = 0
        while len(train_rollouts) < rollouts_per_epoch:
            if _cntr % batch_size == 0:
                typer.echo(f"Step {trainer._global_step_count}: Collected {len(train_rollouts)} of {rollouts_per_epoch} rollouts")

            # Randomply sample propmts from the training set
            prompt = random.choice(train_prompts)
            # Ensure prompt is a single string
            prompt = " ".join(prompt).replace('\n', ' ').replace('\xa0', ' ')

            rollout = trainer.generate_rollout_pair(prompt)

            if rollout is not None:
                train_rollouts.append(rollout)

            _cntr += 1
        typer.echo(f"Step {trainer._global_step_count}: Collected {len(train_rollouts)} of {rollouts_per_epoch} rollouts")

        # Prepare dataset of generated rollouts
        dataset = trainer.prepare_data(train_rollouts, batch_size=batch_size)
        
        # Execute main training step on collected rollouts
        typer.echo(f"Executing training step on {len(train_rollouts)} rollouts")
        trainer.epoch(dataset, log_interval=10)

        # Save model
        trainer.save(str(trainer._global_step_count))

        # Run dev set evaluation 
        if trainer._global_step_count % dev_eval_interval == 0:
            typer.echo(f"Running dev set evaluation on {len(dev_prompts)} prompts")
            
            dev_conversations = []
            for prompt in dev_prompts:
                # Print progress every 10% of the way
                if len(dev_conversations) % (len(dev_prompts) // 10) == 0:
                    typer.echo(f"Progress: {len(dev_conversations)} of {len(dev_prompts)}")

                prompt = " ".join(prompt).replace('\n', ' ').replace('\xa0', ' ')
            

                defender_converation = trainer._generate_rollout(trainer.attacker, trainer.defender, prompt)
                baseline_conversation = trainer._generate_rollout(trainer.attacker, trainer.baseline, prompt)

                dev_conversations.append((defender_converation, baseline_conversation))

            # Get all defender responses
            defender_response_toxicity = []
            baseline_response_toxicity = []

            for dev_conversation in dev_conversations:
                for utterance in dev_conversation[0]:
                    if utterance['role'] == 'def':
                        defender_response_toxicity.append(utterance['toxicity'])

                for utterance in dev_conversation[1]:
                    if utterance['role'] == 'def':
                        baseline_response_toxicity.append(utterance['toxicity'])

            # Compute average toxicity of defender responses in conversation   
            defender_toxicity = np.mean(defender_response_toxicity)
            baseline_toxicity = np.mean(baseline_response_toxicity)

            trainer.accelerator.log({
                "dev/avg_defender_response_toxicity": defender_toxicity,
                "dev/avg_baseline_response_toxicity": baseline_toxicity
            }, step=trainer._global_step_count)

            wandb.log({
                "dev/avg_defender_response_toxicity": defender_toxicity,
                "dev/avg_baseline_response_toxicity": baseline_toxicity
            }, step=trainer._global_step_count)

            typer.echo(f"Completed dev set evaluation")
            typer.echo(f"Average defender response toxicity: {defender_toxicity:.4f}")
            typer.echo(f"Average baseline response toxicity: {baseline_toxicity:.4f}")

            if defender_toxicity < best_model:
                typer.echo(f"New best model found with toxicity: {defender_toxicity:.4f}. Old best toxicity: {best_model:.4f}")

                best_model = defender_toxicity
                trainer.save("best")


if __name__ == "__main__":
    app()
