"""
This is as standalone script to train a baseline model into a hardened defender model based on example rollouts generated using an attacker model
trained using adaptive stress testing.

The general idea is that the traininge examples generated by the AST-attacker model will be more effective at hardening the defender model against
toxicity than the training examples generated by the baseline model.
"""

# Imports
import typer
import json
import copy
import logging
import wandb
from pathlib import Path
from enum import Enum
from dataclasses import dataclass

import numpy as np
import torch
import random

import torch.nn.functional as F
from convokit import Corpus, download
from toxicity.split_data import filter_corpus_by_file
from toxicity.reddit_data_helpers import corpus_to_prompts
from toxicity.dxf import call_detoxify

from accelerate.utils import set_seed
from accelerate.logging import get_logger
from accelerate.utils.tqdm import tqdm
from accelerate import Accelerator
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import LambdaLR
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

from lm import LanguageModel

logger = get_logger("ast")
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
logging.basicConfig(format=LOG_FORMAT, level=logging.INFO)
logger.setLevel(logging.INFO)

# Define Main Typer Function
app = typer.Typer()

# Enum argument for po-method (dpo or apo)
class PoMethod(str, Enum):
    dpo = "dpo"
    apo = "apo"

class ModelName(str, Enum):
    # Define moreb baseline / defender models here. Must be valid Hugging Face model names
    gpt2 = "openai-community/gpt2"

class Attackers(str, Enum):
    # Define more attacker names here. Must be present in ./models as a folder
    gpt2 = "gpt2_defense_gpt2_best"

@dataclass
class Rollout:
    toxic_text: str
    nontoxic_text: str
    toxic_score: float
    nontoxic_score: float

class RolloutDataset(Dataset):
    def __init__(self, rollouts):
        self.rollouts = rollouts

    def __len__(self):
        return len(self.rollouts)

    def __getitem__(self, idx):
        # We just directly return the rollout objects
        # Which contain (toxic_text, nontoxic_text, toxic_score, nontoxic_score)
        return self.rollouts[idx]

class Trainer:
    def __init__(self, 
                 attacker: str, 
                 baseline: str, 
                 defender: str, 
                 wandb: bool, 
                 po_method: PoMethod, 
                 num_turns: int = 3, 
                 po_beta: float = 0.1,
                 learning_rate: float = 5e-7,
                 max_gradient_norm: float = 10.0,
                 warmup_steps: int = 150,
                 save_dir: str = 'models'):
    
        self.num_turns = num_turns

        self.accelerator = Accelerator()

        # The attacker is the AST-pretrained model that tries to get a baseline or defenderm model to generate toxic text
        self.attacker = LanguageModel(dont_init=True)
        self.attacker.model = AutoModelForCausalLM.from_pretrained(attacker)
        self.attacker.tokenizer = AutoTokenizer.from_pretrained(attacker, padding_side='left')

        # The defender is the model we will tune to harden against the attacker
        self.defender = LanguageModel(dont_init=True)
        self.defender.model = AutoModelForCausalLM.from_pretrained(defender)
        self.defender.tokenizer = AutoTokenizer.from_pretrained(defender, padding_side='left')

        # The baseline is the model we will use to generate training examples for the defender
        self.baseline = LanguageModel(dont_init=True)
        self.baseline.model = AutoModelForCausalLM.from_pretrained(baseline)
        self.baseline.tokenizer = AutoTokenizer.from_pretrained(baseline, padding_side='left')

        # GPT 2 doesn't have a padding token, so we add it
        if 'gpt2' in attacker:
            self.attacker.tokenizer.pad_token = self.attacker.tokenizer.eos_token
            self.attacker.tokenizer.pad_token_id = self.attacker.tokenizer.eos_token_id

        if 'gpt2' in defender:
            self.defender.tokenizer.pad_token = self.defender.tokenizer.eos_token
            self.defender.tokenizer.pad_token_id = self.defender.tokenizer.eos_token_id

        if 'gpt2' in baseline:
            self.baseline.tokenizer.pad_token = self.baseline.tokenizer.eos_token
            self.baseline.tokenizer.pad_token_id = self.baseline.tokenizer.eos_token_id

        # Set Adversary and Baseline to eval mode
        self.attacker.model.eval()
        self.baseline.model.eval()

        # Set the policy optimization method
        self.po_method = po_method
        self.po_beta = po_beta
        self.max_gradient_norm = max_gradient_norm

        optimizer = AdamW(self.defender.model.parameters(), lr=learning_rate)
        scheduler = LambdaLR(optimizer, lr_lambda=lambda step: min(1.0, (step + 1) / (warmup_steps + 1)))

        # Use Accelerate to move models to accelrator device. We just let it take care of it
        # This is magic I don't like
        self.defender.model, self.optimizer, self.scheduler = self.accelerator.prepare(self.defender.model, optimizer, scheduler)

        if wandb:
            wandb.watch(self.defender.model)

        self.save_dir = save_dir

        self._global_step_count = 0

    @property
    def device(self):
        return self.accelerator.device

    def finish(self):
        self.accelerator.end_training()

    def save(self, suffix: str | None = None):
        """
        Save the defender model
        """

        filename = self.save_dir + "model_" + suffix if suffix is not None else str(self._global_step_count)

        self.defender.model.save_pretrained(filename)
        self.defender.tokenizer.save_pretrained(filename)

    def generate_rollout_pair(self, prompt: str, toxicity_difference_threshold: float = 0.2, retries_left: int = 3):
        # Generate two rollouts first expected to be toxic, second expected to be nontoxic
        ro_toxic    = self._generate_rollout(self.attacker, self.baseline, prompt)
        ro_nontoxic = self._generate_rollout(self.baseline, self.defender, prompt)

        # Score each of the rollouts
        ro_toxic_score    = self._reward(ro_toxic)
        ro_nontoxic_score = self._reward(ro_nontoxic)

        if abs(ro_toxic_score - ro_nontoxic_score) < toxicity_difference_threshold and retries_left > 0:
            # If the difference in toxicity is not significant, try again
            return self.generate_rollout_pair(prompt, toxicity_difference_threshold, retries_left - 1)
        elif abs(ro_toxic_score - ro_nontoxic_score) < toxicity_difference_threshold and retries_left == 0:
            # If we have tried too many times and still can't get a good pair, return None
            return None
        else:
            # If the difference in toxicity is significant, return the pair and scores
            return Rollout(ro_toxic, ro_nontoxic, ro_toxic_score, ro_nontoxic_score)

    def _generate_rollout(self, attacker_model, defender_model, prompt):

        # Copy the promp
        text = copy.deepcopy(prompt)

        for _ in range(self.num_turns):
            # Generate next turn of attacker model - Prompt appended to start
            text = attacker_model.rollout(text, max_new_tokens=24, repetition_penalty=1.1, temperature=0.7, skip_special_tokens=True, min_new_tokens=2)

            # Generate next turn of defender model
            text = defender_model.rollout(text, max_new_tokens=24, repetition_penalty=1.1, temperature=0.7, skip_special_tokens=True, min_new_tokens=2)

        return text
    
    def _reward(self, text: str):
        """
        Reward function to score the toxicity of an utterance
        """
        
        # For this training since we're just trying to harden the model based on perceived toxicity, we don't adjust
        # reward based on which model generated the text or the perplexity of the text. Just the toxicity score
        
        # Lower toxicity score is better so we negate the toxicity score
        print(text)
        print(call_detoxify(text)["toxicity"])
        return -call_detoxify(text)["toxicity"][0]

    def prepare_data(self, rollouts, batch_size: int = 1):
        """
        Prepare list of examples for training the defender model
        """
        
        # Create a dataset from the rollouts
        ds = RolloutDataset(rollouts)

        # Create a DataLoader from the dataset
        # Don't need to shuffle since the rollout prompts are already randomized
        dl = DataLoader(ds, batch_size=batch_size)

        return self.accelerator.prepare(dl)


    def epoch(self, dataloader, log_interval: int = 10):
        """
        Train the defender model for one epoch
        """
        
        for i, batch in enumerate(iter(dataloader)):

            loss, metrics = self._step(batch, log_step=(i % log_interval == 0))

            gn = clip_grad_norm_(self.defender.model.parameters(), self.max_gradient_norm).cpu().item()

            metrics["training/gradient_norm"] = gn

            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

            if i % log_interval == 0:
                metrics["training/lr"] = self.optimizer.param_groups[0]["lr"]
                self.accelerator.log(metrics, step = self._global_step_count)
                logger.info(f'Step {self._global_step_count} - Reward margin: {round(metrics['rewards/reward_margin'],5):.5f}, Loss: {round(metrics['training/loss'],5):.5f}')

    def _step(self, batch, log_step: bool = False):
        """
        Perform a single training step
        """
        
        toxic_rollout, nontoxic_rollout, _, _ = batch

        # TODO: Discuss the win/loss defender/attacker terminology with Jack

        # Individually calculate the logprobs of the toxic and baseline examples
        with torch.inference_mode():
            baseline_nontoxic_logprobs = self.baseline.logprob_batched(nontoxic_rollout, self.accelerator.device)
            baseline_toxic_logprobs    = self.baseline.logprob_batched(toxic_rollout, self.accelerator.device)

        defender_nontoxic_logprobs = self.defender.logprob_batched(nontoxic_rollout, self.accelerator.device)
        defender_toxic_logprobs = self.defender.logprob_batched(toxic_rollout, self.accelerator.device)

        # Calculate Loss
        losses, preferred_reward, rejected_rewards = self._loss(defender_nontoxic_logprobs, defender_toxic_logprobs,
                                                              baseline_nontoxic_logprobs, baseline_toxic_logprobs)
        reward_accuracies = (preferred_reward > rejected_rewards).float()

        if torch.isnan(losses.mean()):
            raise ValueError("Loss is NaN")
        
        # Calculate metrics
        metrics = {
            "rewards/chosen": preferred_reward.mean().cpu().item(),
            "rewards/rejected": rejected_rewards.mean().cpu().item(),
            "rewards/reward_accuracy": reward_accuracies.mean().cpu().item(),
            "rewards/reward_margin": (preferred_reward - rejected_rewards).mean().cpu().item(),
            "policy/logprobs_preferred": defender_nontoxic_logprobs.mean().detach().cpu().item(),
            "policy/logprobs_rejected": defender_toxic_logprobs.mean().detach().cpu().item(),
            "ref/logprobs_preferred": baseline_nontoxic_logprobs.mean().detach().cpu().item(),
            "ref/logprobs_rejected": baseline_toxic_logprobs.mean().detach().cpu().item(),
            "training/loss": losses.mean().detach().cpu().item(),
            "debug/text": wandb.Table(data=list(zip(nontoxic_rollout, toxic_rollout)), 
                columns=["chosen", "rejected"])
        }

        return losses.mean(), metrics

    def _loss(self, policy_chosen_logps, policy_rejected_logps,
                     reference_chosen_logps, reference_rejected_logps):
        
        # https://github.com/eric-mitchell/direct-preference-optimization/blob/f8b8c0f49dc92a430bae41585f9d467d3618fe2f/trainers.py#L70-L87
        
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios  # also known as h_{\pi_\theta}^{y_w,y_l}

        # this is IPO, Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf
        if self.po_method == PoMethod.dpo:
            losses = -F.logsigmoid(self.beta * logits) * (1 - self.args.label_smooth) - F.logsigmoid(-self.beta * logits) * self.args.label_smooth
        else: # IPO
            losses = (logits - 1/(2 * self.beta)) ** 2

        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()

        return losses, chosen_rewards, rejected_rewards

@app.command()
def train_defender(
    save_dir: str = 'models',
    save_name: str = typer.Argument(...),
    attacker: Attackers = Attackers.gpt2,
    baseline: ModelName = ModelName.gpt2,
    defender: ModelName = ModelName.gpt2,
    num_turns: int = typer.Option(3, help="Number of turns in the conversation"),
    epochs: int = typer.Option(10000, help="Number of epochs to train the defender model"),
    wandb: bool = typer.Option(False, help="Whether to log to wandb"),
    po_method: PoMethod = typer.Option(PoMethod.dpo, help="The policy optimization method to use"),
    po_beta: float = typer.Option(0.1, help="The beta parameter for the DPO objective"),
    batch_size: int = typer.Option(1, help="Batch size for training the defender model"), # 8
    learning_rate: float = typer.Option(5e-7, help="Learning rate for the defender model"),
    warmup_steps: int = typer.Option(150, help="Number of warmup steps for the learning rate scheduler"),
    rollouts_per_epoch: int = typer.Option(1, help="Number of rollouts to generate per epoch"), # 256
    seed: int = typer.Option(42, help="Random seed for reproducibility"),
):
    """
    Train a baseline model into a hardened defender model based on example rollouts generated using an attacker model
    trained using adaptive stress testing.
    """
    
    # Create a new directory for the model if it does not exist
    output_dir = Path(save_dir) / save_name

    if not output_dir.exists():
        output_dir.mkdir(parents=True)

    typer.echo(f"Saving model to {output_dir}")

    # Save the configuration to the output directory
    config = {
        "attacker": attacker,
        "baseline": baseline,
        "defender": defender,
        "epochs": epochs,
        "wandb": wandb,
        "po_method": po_method,
        "seed": seed,
        "prompt_source": "convokit",
        "num_turns": num_turns,
        "learning_rate": learning_rate,
        "warmup_steps": warmup_steps,
        "batch_size": batch_size,
    }

    with open(output_dir / "config.json", "w") as f:
        json.dump(config, f, indent=4)

    # Set the seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    set_seed(seed)

    # Load data
    typer.echo("Loading data...")
    train_prompts = corpus_to_prompts(filter_corpus_by_file(Corpus(filename=download("reddit-corpus-small")), "data/train.txt"))
    dev_prompts   = corpus_to_prompts(filter_corpus_by_file(Corpus(filename=download("reddit-corpus-small")), "data/dev.txt"))

    # Initialize Training Model
    typer.echo("Initializing models...")

    typer.echo(f"Attacker: {attacker.value}")
    typer.echo(f"Baseline: {baseline.value}")
    typer.echo(f"Defender: {defender.value}")

    trainer = Trainer(
        attacker= "./models/" + attacker.value, 
        baseline=baseline.value, 
        defender=defender.value, 
        wandb=wandb, 
        po_method=po_method, 
        num_turns=num_turns, 
        po_beta=po_beta, 
        learning_rate=learning_rate, 
        warmup_steps=warmup_steps,
        save_dir=save_dir
    )

    # Initialize Wand

    epoch = 0
    # while epoch < epochs:
    typer.echo(f"Epoch {epoch}")

    rollouts = []
    _cntr = 0
    while len(rollouts) < rollouts_per_epoch:
        if _cntr % 1 == 0:
            typer.echo(f"Step {epoch}: Collected {len(rollouts)} of {rollouts_per_epoch} rollouts")

        # Randomply sample propmts from the training set
        prompt = random.choice(train_prompts)
        # Ensure prompt is a single string
        prompt = [" ".join(prompt).replace('\n', ' ')]
        print(f'PROMPT: {prompt}')
        rollouts.append(trainer.generate_rollout_pair(prompt))
        
    print(rollouts)

    # Prepare dataset of generated rollouts
    dataset = trainer.preapare_data(rollouts, batch_size=batch_size)

    print(dataset)
    
    # Execute main training step on collected rollouts
    # typer.echo(f"Executing training step on {len(rollouts)} rollouts")
    # trainer.epoch(dataset, log_interval=10)

    epoch += 1



if __name__ == "__main__":
    app()
